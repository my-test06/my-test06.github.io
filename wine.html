<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Wine Data Analysis &mdash; SciBits.org</title>
  <meta name="author" content="Mangs">

  <link rel="canonical" href="/wine.html"/>
  
  <meta property="og:site_name" content="SciBits.org" />
  <meta property="og:type" content="article" />
    
  <meta property="og:title" content="Wine Data Analysis" />
  <meta property="og:url" content="/wine.html" />
  <meta property="og:description" content="To analyze the information given in the 'Wines' dataset. Classify the wines based on their place of origin. This data contains the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines." />
  <meta property="article:published_time" content="2017-12-30 10:20:00-08:00" />
  <meta property="article:modified_time" content="2017-12-30 10:20:00-08:00" />






  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="/favicon.png" rel="icon">

  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">

  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>
</head>

<body >
  <header role="banner"
  >
<hgroup>
  <h1><a href="/">SciBits.org</a></h1>
</hgroup></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
</ul>


<ul class="main-navigation">
    <li><a href="/archives.html">Archives</a></li>
    <li><a href="/pages/about.html">About</a></li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Wine Data Analysis</h1>
    <p class="meta">
<time datetime="2017-12-30T10:20:00-08:00" pubdate>Sat 30 December 2017</time>      
    </p>
</header>

    <div class="entry-content"><p>To analyze the information given in the 'Wines' dataset. Classify the wines based on their place of origin. This data contains the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. </p>
<p>I found the <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data">data</a> from a study which measured chemical constituents like alcohol, malic acid and various other constituents.</p>
<p>For this analysis, I'm going to use logistic regression, decision tree, random forest, naive bayes model , SVM and GBM from the dataset and compare the classification accuracy. Later I will use grid search approach to identify best model tuning parameters and compare the model performance</p>
<p>First we'll start by importing some packages and then import the data. </p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">sklearn.ensemble</span> <span class="kn">as</span> <span class="nn">ske</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="c1">#from IPython.display import Image</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">,</span><span class="n">log_loss</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">GradientBoostingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">decomposition</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span><span class="p">,</span> <span class="n">OneHotEncoder</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>

<span class="kn">import</span> <span class="nn">os</span>

<span class="n">labelencoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
</pre></div>


<p>We'll display a snapshot of the data.</p>
<div class="highlight"><pre><span></span><span class="n">wine_data</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">,</span><span class="s1">&#39;Alcohol&#39;</span><span class="p">,</span><span class="s1">&#39;Malicacid&#39;</span><span class="p">,</span><span class="s1">&#39;Ash&#39;</span><span class="p">,</span><span class="s1">&#39;Alcalinityofash&#39;</span><span class="p">,</span><span class="s1">&#39;Magnesium&#39;</span><span class="p">,</span><span class="s1">&#39;Totalphenols&#39;</span><span class="p">,</span><span class="s1">&#39;Flavanoids&#39;</span>
                     <span class="p">,</span><span class="s1">&#39;Nonflavanoidphenols&#39;</span><span class="p">,</span> <span class="s1">&#39;Proanth&#39;</span><span class="p">,</span><span class="s1">&#39;Colorintensity&#39;</span><span class="p">,</span><span class="s1">&#39;Hue&#39;</span><span class="p">,</span><span class="s1">&#39;OD280&#39;</span><span class="p">,</span><span class="s1">&#39;Proline&#39;</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="n">wine_data</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</pre></div>


<div class="highlight"><pre><span></span>   Class  Alcohol  Malicacid   Ash  Alcalinityofash  Magnesium  Totalphenols  \
0      1    13.20       1.78  2.14             11.2        100          2.65   
1      1    13.16       2.36  2.67             18.6        101          2.80   
2      1    14.37       1.95  2.50             16.8        113          3.85   
3      1    13.24       2.59  2.87             21.0        118          2.80   
4      1    14.20       1.76  2.45             15.2        112          3.27

   Flavanoids  Nonflavanoidphenols  Proanth  Colorintensity   Hue  OD280  \
0        2.76                 0.26     1.28            4.38  1.05   3.40   
1        3.24                 0.30     2.81            5.68  1.03   3.17   
2        3.49                 0.24     2.18            7.80  0.86   3.45   
3        2.69                 0.39     1.82            4.32  1.04   2.93   
4        3.39                 0.34     1.97            6.75  1.05   2.85

   Proline  
0     1050  
1     1185  
2     1480  
3      735  
4     1450
</pre></div>


<div class="highlight"><pre><span></span><span class="n">features</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">wine_data</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="n">labelencoder</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">df_labelencoded</span> <span class="o">=</span> <span class="n">wine_data</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">labelencoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">df_labelencoded</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</pre></div>


<div class="highlight"><pre><span></span>   Class  Alcohol  Malicacid  Ash  Alcalinityofash  Magnesium  Totalphenols  \
0      0       65         49   18                1         21            68   
1      0       63         71   62               31         22            73   
2      0      120         59   48               20         33            95   
3      0       67         81   74               44         37            73   
4      0      115         47   45               10         32            89

   Flavanoids  Nonflavanoidphenols  Proanth  Colorintensity  Hue  OD280  \
0          91                    9       31              56   50    102   
1         114                   13       96              82   48     86   
2         121                    7       84             108   30    104   
3          89                   20       66              53   49     73   
4         119                   17       75              96   50     68

   Proline  
0       92  
1      101  
2      116  
3       66  
4      115
</pre></div>


<p>We'll make some plots to see the data available to us.</p>
<div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">9</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">18</span>

<span class="n">wine_data</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">column</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Alcohol&#39;</span><span class="p">,</span><span class="s1">&#39;Malicacid&#39;</span><span class="p">,</span><span class="s1">&#39;Ash&#39;</span><span class="p">,</span><span class="s1">&#39;Alcalinityofash&#39;</span><span class="p">],</span>
            <span class="n">by</span><span class="o">=</span><span class="s1">&#39;Class&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000C6E1E75B70&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000C6E2131940&gt;],
       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000C6E21581D0&gt;,
        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000C6E2173240&gt;]], dtype=object)
</pre></div>


<p><img alt="png" src="/images/wine_8_1.png"></p>
<div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">11</span>
<span class="n">wine_data</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">column</span><span class="o">=</span><span class="s1">&#39;Alcohol&#39;</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="s1">&#39;Class&#39;</span><span class="p">,</span><span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">layout</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000C6E24A09B0&gt;,
       &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000C6E22BDA90&gt;,
       &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000000C6E22E1940&gt;], dtype=object)
</pre></div>


<p><img alt="png" src="/images/wine_9_1.png"></p>
<div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">lmplot</span><span class="p">(</span><span class="s1">&#39;Alcohol&#39;</span><span class="p">,</span><span class="s1">&#39;Ash&#39;</span><span class="p">,</span><span class="n">wine_data</span><span class="p">,</span><span class="n">hue</span><span class="o">=</span><span class="s1">&#39;Class&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.FacetGrid at 0xc6e2481da0&gt;
</pre></div>


<p><img alt="png" src="/images/wine_10_1.png"></p>
<div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">lmplot</span><span class="p">(</span><span class="s1">&#39;Alcohol&#39;</span><span class="p">,</span><span class="s1">&#39;Ash&#39;</span><span class="p">,</span><span class="n">wine_data</span><span class="p">,</span><span class="n">hue</span><span class="o">=</span><span class="s1">&#39;Class&#39;</span><span class="p">,</span><span class="n">fit_reg</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.FacetGrid at 0xc6e27dceb8&gt;
</pre></div>


<p><img alt="png" src="/images/wine_11_1.png"></p>
<div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;Class&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;Alcohol&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">wine_data</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>&lt;matplotlib.axes._subplots.AxesSubplot at 0xc6e266d4a8&gt;
</pre></div>


<p><img alt="png" src="/images/wine_12_1.png"></p>
<div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">wine_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">13</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">wine_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;----------y values------------&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>----------y values------------
[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]
</pre></div>


<p>Here we'll split the data into a training set and a testing set.</p>
<div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Feature Scaling</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">model_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Model&#39;</span><span class="p">,</span> <span class="s1">&#39;Accuracy Score&#39;</span><span class="p">,</span><span class="s1">&#39;Log loss&#39;</span><span class="p">]</span>
<span class="n">model_log</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">model_cols</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Applying PCA</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;-------------Applying PCA-------------&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">explained_variance</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;------------Explained Variance------------&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">explained_variance</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;------------Covariance------------&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">get_covariance</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;------------Precision------------&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">get_precision</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>-------------Applying PCA-------------
------------Explained Variance------------
[ 0.37362934  0.18399859]
------------Covariance------------
&lt;bound method _BasePCA.get_covariance of PCA(copy=True, iterated_power=&#39;auto&#39;, n_components=2, random_state=None,
  svd_solver=&#39;auto&#39;, tol=0.0, whiten=False)&gt;
------------Precision------------
&lt;bound method _BasePCA.get_precision of PCA(copy=True, iterated_power=&#39;auto&#39;, n_components=2, random_state=None,
  svd_solver=&#39;auto&#39;, tol=0.0, whiten=False)&gt;
</pre></div>


<div class="highlight"><pre><span></span><span class="n">model_log_entry</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([[</span><span class="s1">&#39;PCA &#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span><span class="s1">&#39;&#39;</span><span class="p">]],</span> <span class="n">columns</span><span class="o">=</span><span class="n">model_cols</span><span class="p">)</span>
<span class="n">model_log_entry</span>
<span class="n">model_log</span> <span class="o">=</span> <span class="n">model_log</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model_log_entry</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Fitting Logistic Regression to the Training set</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predicting the Test set results</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Making the Confusion Matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;-------------Confusion Matrix------------&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">cm</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;-----------accuracy score-------------&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="n">acs</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="n">model_log_entry</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([[</span><span class="s1">&#39;Logistic Regression &#39;</span><span class="p">,</span> 
                                 <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">),</span>
                                 <span class="s2">&quot;&quot;</span><span class="p">]],</span> <span class="n">columns</span><span class="o">=</span><span class="n">model_cols</span><span class="p">)</span>
<span class="n">model_log</span> <span class="o">=</span> <span class="n">model_log</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model_log_entry</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>-------------Confusion Matrix------------
[[12  2  0]
 [ 0 14  1]
 [ 0  0  7]]
-----------accuracy score-------------
0.916666666667
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Visualising the Training set results</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="n">X_set</span><span class="p">,</span> <span class="n">y_set</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span>
<span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span> <span class="o">=</span> <span class="n">X_set</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="n">X_set</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">),</span>
                      <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span> <span class="o">=</span> <span class="n">X_set</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="n">X_set</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">X1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">X2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X1</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
              <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">((</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">X1</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X1</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">X2</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X2</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_set</span><span class="p">)):</span>
     <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_set</span><span class="p">[</span><span class="n">y_set</span> <span class="o">==</span> <span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_set</span><span class="p">[</span><span class="n">y_set</span> <span class="o">==</span> <span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                 <span class="n">c</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">((</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">))(</span><span class="n">i</span><span class="p">),</span> <span class="n">label</span> <span class="o">=</span> <span class="n">j</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Logistic Regression (Training set)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;PC1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;PC2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="/images/wine_20_0.png"></p>
<div class="highlight"><pre><span></span><span class="c1"># Visualising the Test set results</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span>
<span class="n">X_set</span><span class="p">,</span> <span class="n">y_set</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span>
<span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span> <span class="o">=</span> <span class="n">X_set</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="n">X_set</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">),</span>
                     <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span> <span class="o">=</span> <span class="n">X_set</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="n">X_set</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">step</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">X1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">X2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X1</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
             <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">((</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">X1</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X1</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">X2</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">X2</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_set</span><span class="p">)):</span>
     <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_set</span><span class="p">[</span><span class="n">y_set</span> <span class="o">==</span> <span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_set</span><span class="p">[</span><span class="n">y_set</span> <span class="o">==</span> <span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                 <span class="n">c</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">((</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">))(</span><span class="n">i</span><span class="p">),</span> <span class="n">label</span> <span class="o">=</span> <span class="n">j</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Logistic Regression (Test set)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;PC1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;PC2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="/images/wine_21_0.png"></p>
<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s2">&quot;-------------Applying Decision Tree------------&quot;</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">dtree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">dtree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">Y_pred</span> <span class="o">=</span> <span class="n">dtree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">cmDT</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Decision Tree confusion matrix and accuracy score&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">cmDT</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">))</span>
<span class="n">model_log_entry</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([[</span><span class="s1">&#39;Decision Tree &#39;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">),</span><span class="s1">&#39;&#39;</span><span class="p">]],</span> <span class="n">columns</span><span class="o">=</span><span class="n">model_cols</span><span class="p">)</span>
<span class="n">model_log</span> <span class="o">=</span> <span class="n">model_log</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model_log_entry</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>-------------Applying Decision Tree------------
Decision Tree confusion matrix and accuracy score
[[14  0  0]
 [ 1 12  2]
 [ 0  0  7]]
0.916666666667
</pre></div>


<div class="highlight"><pre><span></span><span class="n">tree</span><span class="o">.</span><span class="n">export_graphviz</span><span class="p">(</span><span class="n">dtree</span><span class="p">,</span> <span class="n">out_file</span> <span class="o">=</span> <span class="s1">&#39;wine1_dtree22.dot&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">popen</span><span class="p">(</span><span class="s1">&#39;dot &quot;wine1_dtree22.dot&quot; -Tpng -o &quot;wine1_dtree22.png&quot;&#39;</span><span class="p">)</span>
<span class="n">dtimg</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s2">&quot;wine1_dtree22.png&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">dtimg</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage at 0xc6e3e616d8&gt;
</pre></div>


<p><img alt="png" src="/images/wine_24_1.png"></p>
<p>Next, we'll go through different classifiers and list the Accuracy and Log Loss for each classifier.</p>
<div class="highlight"><pre><span></span><span class="c1">#log for visual comparison</span>
<span class="n">log_cols</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Classifier&quot;</span><span class="p">,</span> <span class="s2">&quot;Accuracy&quot;</span><span class="p">,</span> <span class="s2">&quot;Log Loss&quot;</span><span class="p">]</span>
<span class="n">basic_log</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">log_cols</span><span class="p">)</span>
<span class="n">best_model_log</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">log_cols</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1">#Initializing classifiers with default</span>
<span class="n">basic_classifiers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">KNeighborsClassifier</span><span class="p">(),</span>
    <span class="n">SVC</span><span class="p">(</span><span class="n">probability</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">),</span>
    <span class="n">DecisionTreeClassifier</span><span class="p">(),</span>
    <span class="n">AdaBoostClassifier</span><span class="p">(),</span>
    <span class="n">RandomForestClassifier</span><span class="p">(),</span>
    <span class="n">GradientBoostingClassifier</span><span class="p">(),</span>
    <span class="n">GaussianNB</span><span class="p">()]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1">#print classifiers</span>
<span class="n">basic_classifiers</span>
</pre></div>


<div class="highlight"><pre><span></span>[KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
            metric_params=None, n_jobs=1, n_neighbors=5, p=2,
            weights=&#39;uniform&#39;),
 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
   decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;rbf&#39;,
   max_iter=-1, probability=True, random_state=None, shrinking=True,
   tol=0.001, verbose=False),
 LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
           intercept_scaling=1, max_iter=100, multi_class=&#39;ovr&#39;, n_jobs=1,
           penalty=&#39;l2&#39;, random_state=0, solver=&#39;liblinear&#39;, tol=0.0001,
           verbose=0, warm_start=False),
 DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=None,
             max_features=None, max_leaf_nodes=None,
             min_impurity_decrease=0.0, min_impurity_split=None,
             min_samples_leaf=1, min_samples_split=2,
             min_weight_fraction_leaf=0.0, presort=False, random_state=None,
             splitter=&#39;best&#39;),
 AdaBoostClassifier(algorithm=&#39;SAMME.R&#39;, base_estimator=None,
           learning_rate=1.0, n_estimators=50, random_state=None),
 RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,
             max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None,
             min_impurity_decrease=0.0, min_impurity_split=None,
             min_samples_leaf=1, min_samples_split=2,
             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
             oob_score=False, random_state=None, verbose=0,
             warm_start=False),
 GradientBoostingClassifier(criterion=&#39;friedman_mse&#39;, init=None,
               learning_rate=0.1, loss=&#39;deviance&#39;, max_depth=3,
               max_features=None, max_leaf_nodes=None,
               min_impurity_decrease=0.0, min_impurity_split=None,
               min_samples_leaf=1, min_samples_split=2,
               min_weight_fraction_leaf=0.0, n_estimators=100,
               presort=&#39;auto&#39;, random_state=None, subsample=1.0, verbose=0,
               warm_start=False),
 GaussianNB(priors=None)]
</pre></div>


<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s2">&quot;Printing results from base classifier&quot;</span><span class="p">)</span>
<span class="n">Xtrain</span><span class="p">,</span> <span class="n">Xtest</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">ytest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">for</span> <span class="n">clf</span> <span class="ow">in</span> <span class="n">basic_classifiers</span><span class="p">:</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">30</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;****Results****&#39;</span><span class="p">)</span>
    <span class="n">train_predictions</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">train_predictions</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: {:.4%}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">acc</span><span class="p">))</span>

    <span class="n">train_predictions</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)</span>
    <span class="n">ll</span> <span class="o">=</span> <span class="n">log_loss</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">train_predictions</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Log Loss: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ll</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">30</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Printing results from base classifier
==============================
KNeighborsClassifier
****Results****
Accuracy: 83.3333%
Log Loss: 2.19159634006964
==============================
SVC
****Results****
Accuracy: 86.1111%
Log Loss: 0.30756417482227505
==============================
LogisticRegression
****Results****
Accuracy: 91.6667%
Log Loss: 0.24735412308073323
==============================
DecisionTreeClassifier
****Results****
Accuracy: 91.6667%
Log Loss: 2.878231366242558
==============================
AdaBoostClassifier
****Results****
Accuracy: 86.1111%
Log Loss: 0.6466171620181561
==============================
RandomForestClassifier
****Results****
Accuracy: 97.2222%
Log Loss: 0.18896288822511523
==============================
GradientBoostingClassifier
****Results****
Accuracy: 91.6667%
Log Loss: 0.11083595399697523
==============================
GaussianNB
****Results****
Accuracy: 91.6667%
Log Loss: 0.08836397292012357
==============================
</pre></div>


<p>I will run a grid search for each of the classifiers and display the best hyper parameters for each classifier.</p>
<div class="highlight"><pre><span></span><span class="c1">#best model classifiers using variables</span>
<span class="n">best_model_classifiers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">gs_knn_classifier</span><span class="p">,</span>
    <span class="n">gs_svm_classifier</span><span class="p">,</span>
    <span class="n">gs_logreg_classifier</span><span class="p">,</span>
    <span class="n">gs_dtree_classifier</span><span class="p">,</span>
    <span class="n">gs_ab_classifier</span><span class="p">,</span>
    <span class="n">gs_rf_classifier</span><span class="p">,</span>
    <span class="n">gs_gb_classifier</span><span class="p">,</span>
    <span class="n">gs_gnb_classifier</span>
<span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">best_model_classifiers</span>
</pre></div>


<div class="highlight"><pre><span></span>[KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
            metric_params=None, n_jobs=1, n_neighbors=3, p=2,
            weights=&#39;uniform&#39;),
 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
   decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;linear&#39;,
   max_iter=-1, probability=True, random_state=None, shrinking=True,
   tol=0.001, verbose=False),
 LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
           intercept_scaling=1, max_iter=100, multi_class=&#39;ovr&#39;, n_jobs=1,
           penalty=&#39;l2&#39;, random_state=0, solver=&#39;liblinear&#39;, tol=0.0001,
           verbose=0, warm_start=False),
 DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=5,
             max_features=None, max_leaf_nodes=None,
             min_impurity_decrease=0.0, min_impurity_split=None,
             min_samples_leaf=1, min_samples_split=2,
             min_weight_fraction_leaf=0.0, presort=False, random_state=None,
             splitter=&#39;best&#39;),
 AdaBoostClassifier(algorithm=&#39;SAMME&#39;, base_estimator=None, learning_rate=1.0,
           n_estimators=100, random_state=None),
 RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,
             max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None,
             min_impurity_decrease=0.0, min_impurity_split=None,
             min_samples_leaf=1, min_samples_split=2,
             min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,
             oob_score=False, random_state=None, verbose=0,
             warm_start=False),
 GradientBoostingClassifier(criterion=&#39;friedman_mse&#39;, init=None,
               learning_rate=0.1, loss=&#39;deviance&#39;, max_depth=3,
               max_features=None, max_leaf_nodes=None,
               min_impurity_decrease=0.0, min_impurity_split=None,
               min_samples_leaf=1, min_samples_split=2,
               min_weight_fraction_leaf=0.0, n_estimators=100,
               presort=&#39;auto&#39;, random_state=None, subsample=1.0, verbose=0,
               warm_start=False),
 GaussianNB(priors=None)]
</pre></div>


<p>Printing results using best hyper parameters from grid search using variables.</p>
<div class="highlight"><pre><span></span>Printing results using best hyper parameters from grid search using variables
==============================
KNeighborsClassifier
****Results****
Accuracy: 80.5556%
Log Loss: 2.1164576295998505
==============================
SVC
****Results****
Accuracy: 94.4444%
Log Loss: 0.15336951382205888
==============================
LogisticRegression
****Results****
Accuracy: 91.6667%
Log Loss: 0.24735412308073323
==============================
DecisionTreeClassifier
****Results****
Accuracy: 91.6667%
Log Loss: 2.878231366242558
==============================
AdaBoostClassifier
****Results****
Accuracy: 91.6667%
Log Loss: 1.0006315027859083
==============================
RandomForestClassifier
****Results****
Accuracy: 97.2222%
Log Loss: 0.12975238096954453
==============================
GradientBoostingClassifier
****Results****
Accuracy: 91.6667%
Log Loss: 0.1172436075713107
==============================
GaussianNB
****Results****
Accuracy: 91.6667%
Log Loss: 0.08836397292012357
==============================
</pre></div>


<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s2">&quot;------------------Individual run----------------&quot;</span><span class="p">)</span>
<span class="n">model_log</span>
</pre></div>


<div class="highlight"><pre><span></span>------------------Individual run----------------
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model</th>
      <th>Accuracy Score</th>
      <th>Log loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>PCA</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>1</th>
      <td>Logistic Regression</td>
      <td>0.916667</td>
      <td></td>
    </tr>
    <tr>
      <th>2</th>
      <td>Decision Tree</td>
      <td>0.916667</td>
      <td></td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s2">&quot;-----------Base Classifiers--------------&quot;</span><span class="p">)</span>
<span class="n">basic_log</span>
</pre></div>


<div class="highlight"><pre><span></span>-----------Base Classifiers--------------
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Classifier</th>
      <th>Accuracy</th>
      <th>Log Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>KNeighborsClassifier</td>
      <td>83.333333</td>
      <td>2.191596</td>
    </tr>
    <tr>
      <th>1</th>
      <td>SVC</td>
      <td>86.111111</td>
      <td>0.307564</td>
    </tr>
    <tr>
      <th>2</th>
      <td>LogisticRegression</td>
      <td>91.666667</td>
      <td>0.247354</td>
    </tr>
    <tr>
      <th>3</th>
      <td>DecisionTreeClassifier</td>
      <td>91.666667</td>
      <td>2.878231</td>
    </tr>
    <tr>
      <th>4</th>
      <td>AdaBoostClassifier</td>
      <td>86.111111</td>
      <td>0.646617</td>
    </tr>
    <tr>
      <th>5</th>
      <td>RandomForestClassifier</td>
      <td>97.222222</td>
      <td>0.188963</td>
    </tr>
    <tr>
      <th>6</th>
      <td>GradientBoostingClassifier</td>
      <td>91.666667</td>
      <td>0.110836</td>
    </tr>
    <tr>
      <th>7</th>
      <td>GaussianNB</td>
      <td>91.666667</td>
      <td>0.088364</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_color_codes</span><span class="p">(</span><span class="s2">&quot;muted&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Classifier&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">basic_log</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span> <span class="o">=</span> <span class="s2">&quot;Accuracy %&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;Classifier Accuracy&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="/images/wine_51_0.png"></p>
<div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_color_codes</span><span class="p">(</span><span class="s2">&quot;muted&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;Log Loss&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Classifier&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">basic_log</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span> <span class="o">=</span> <span class="s2">&quot;Log Loss %&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;Classifier Log Loss&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="/images/wine_52_0.png"></p>
<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s2">&quot;----best model taken into variables for grid search-----&quot;</span><span class="p">)</span>
<span class="n">best_model_log</span>
</pre></div>


<div class="highlight"><pre><span></span>----best model taken into variables for grid search-----
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Classifier</th>
      <th>Accuracy</th>
      <th>Log Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>KNeighborsClassifier</td>
      <td>80.555556</td>
      <td>2.116458</td>
    </tr>
    <tr>
      <th>1</th>
      <td>SVC</td>
      <td>94.444444</td>
      <td>0.153370</td>
    </tr>
    <tr>
      <th>2</th>
      <td>LogisticRegression</td>
      <td>91.666667</td>
      <td>0.247354</td>
    </tr>
    <tr>
      <th>3</th>
      <td>DecisionTreeClassifier</td>
      <td>91.666667</td>
      <td>2.878231</td>
    </tr>
    <tr>
      <th>4</th>
      <td>AdaBoostClassifier</td>
      <td>91.666667</td>
      <td>1.000632</td>
    </tr>
    <tr>
      <th>5</th>
      <td>RandomForestClassifier</td>
      <td>97.222222</td>
      <td>0.129752</td>
    </tr>
    <tr>
      <th>6</th>
      <td>GradientBoostingClassifier</td>
      <td>91.666667</td>
      <td>0.117244</td>
    </tr>
    <tr>
      <th>7</th>
      <td>GaussianNB</td>
      <td>91.666667</td>
      <td>0.088364</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_color_codes</span><span class="p">(</span><span class="s2">&quot;muted&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Classifier&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">best_model_log</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span> <span class="o">=</span> <span class="s2">&quot;Accuracy %&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;Classifier Accuracy from Grid Search&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="/images/wine_54_0.png"></p>
<div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">font</span> <span class="o">=</span> <span class="mi">14</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_color_codes</span><span class="p">(</span><span class="s2">&quot;muted&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;Log Loss&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Classifier&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">best_model_log</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span> <span class="o">=</span> <span class="s2">&quot;Log Loss %&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;Classifier Accuracy&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="/images/wine_55_0.png"></p>
<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s2">&quot;Basic classifiers </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="p">(</span><span class="n">basic_log</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> Best model classifiers using grid search </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">best_model_log</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Basic classifiers

                   Classifier   Accuracy  Log Loss
0        KNeighborsClassifier  83.333333  2.191596
0                         SVC  86.111111  0.307564
0          LogisticRegression  91.666667  0.247354
0      DecisionTreeClassifier  91.666667  2.878231
0          AdaBoostClassifier  86.111111  0.646617
0      RandomForestClassifier  97.222222  0.188963
0  GradientBoostingClassifier  91.666667  0.110836
0                  GaussianNB  91.666667  0.088364

 Best model classifiers using grid search

                   Classifier   Accuracy  Log Loss
0        KNeighborsClassifier  80.555556  2.116458
0                         SVC  94.444444  0.153370
0          LogisticRegression  91.666667  0.247354
0      DecisionTreeClassifier  91.666667  2.878231
0          AdaBoostClassifier  91.666667  1.000632
0      RandomForestClassifier  97.222222  0.129752
0  GradientBoostingClassifier  91.666667  0.117244
0                  GaussianNB  91.666667  0.088364
</pre></div></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        Mangs
    </span>
  </span>
<time datetime="2017-12-30T10:20:00-08:00" pubdate>Sat 30 December 2017</time>  <span class="categories">
    <a class='category' href='/category/misc.html'>misc</a>
  </span>
  <span class="categories">
    <a class="category" href="/tag/python.html">Python</a>,    <a class="category" href="/tag/data-science.html">Data Science</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>

<aside class="sidebar">
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    <li class="post">
        <a href="/sentiment.html">Sentiment Analysis</a>
    </li>
    <li class="post">
        <a href="/anna.html">Text Generation</a>
    </li>
    <li class="post">
        <a href="/wine.html">Wine Data Analysis</a>
    </li>
    <li class="post">
        <a href="/airpsg.html">Airline Passengers Data Analysis</a>
    </li>
    <li class="post">
        <a href="/tableau.html">Tableau Dashboard</a>
    </li>
  </ul>
</section><section>
  <h1>Categories</h1>
    <ul id="recent_posts">
      <li><a href="/category/misc.html">misc (5)</a></li>
  </ul>
</section>
<section>
  <h1>Tags</h1>
    <a href="/tag/r.html">R</a>,    <a href="/tag/python.html">Python</a>,    <a href="/tag/tableau.html">Tableau</a>,    <a href="/tag/data-science.html">Data Science</a></section>

</aside>
    </div>
  </div>
  <footer role="contentinfo">
<p>
    Copyright &copy;  2017  Mangs &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p>  </footer>
</body>
</html>